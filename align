#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.25, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int", help="Number of sentences to use for training and alignment")
optparser.add_option("-i", "--num_iterates", dest="num_iters", default=10, type="int", help="Number of iterations for model 1")
optparser.add_option("-m", "--mode", dest="mode", default=2, type="int", help="The model to train. 0 for dice, 1 for model1, 2 for symmetric model1. Default: 2")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)


def train_dice():
  sys.stderr.write("Training with Dice's coefficient...")
  full_bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))]
  bitext = full_bitext[:opts.num_sents]
  f_count = defaultdict(int)
  e_count = defaultdict(int)
  fe_count = defaultdict(int)
  for (n, (f, e)) in enumerate(bitext):
    for f_i in set(f):
      f_count[f_i] += 1
      for e_j in set(e):
        fe_count[(f_i,e_j)] += 1
    for e_j in set(e):
      e_count[e_j] += 1
    if n % 500 == 0:
      sys.stderr.write(".")

  dice = defaultdict(int)
  for (k, (f_i, e_j)) in enumerate(fe_count.keys()):
    dice[(f_i,e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])
    if k % 5000 == 0:
      sys.stderr.write(".")
  sys.stderr.write("\n")

  for (f, e) in full_bitext:
    for (i, f_i) in enumerate(f): 
      for (j, e_j) in enumerate(e):
        if dice[(f_i,e_j)] >= opts.threshold:
          sys.stdout.write("%i-%i " % (i,j))
    sys.stdout.write("\n")


def train_model1():
  sys.stderr.write("Training with Model1's coefficient...")
  full_bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))]
  bitext = full_bitext[:opts.num_sents]
  iter_num = opts.num_iters
  t_ef = defaultdict(float)
  
  f_e_map = defaultdict(set)
  for (_, (f, e)) in enumerate(bitext):
    for word_e in e:
        for word_f in f:
          f_e_map[word_f].update(e)
  for (_, (f, e)) in enumerate(bitext):
    for word_e in e:
        for word_f in f:
          t_ef[(word_e,word_f)] = 1/len(f_e_map[word_f])
  
  for iter in range(iter_num):
    sys.stderr.write("\n iter: {}/{}".format(iter+1, iter_num))
    sys.stderr.write("\n Updating count_ef ")
    total_f = defaultdict(float)
    count_ef = defaultdict(float)
    s_total = defaultdict(float)
    for (n, (f, e)) in enumerate(bitext):
      for word_e in e:
        for word_f in f:
          s_total[word_e] += t_ef[(word_e,word_f)]
      for word_e in e:
        for word_f in f:
          prob = t_ef[(word_e,word_f)]/(s_total[word_e])
          count_ef[(word_e,word_f)] += prob
          total_f[(word_f)] += prob
    
    sys.stderr.write("\n Updating t_ef ")
    for (word_e, word_f) in count_ef.keys():
      t_ef[(word_e,word_f)] = count_ef[(word_e,word_f)]/(total_f[(word_f)])

    sys.stderr.write("\n")

  for (f, e) in full_bitext:
    for (i, f_i) in enumerate(f): 
      for (j, e_j) in enumerate(e):
        
        if t_ef[(e_j,f_i)] >= opts.threshold:
          sys.stdout.write("%i-%i " % (i,j))
    sys.stdout.write("\n")


def train_symmetric_model1():
  sys.stderr.write("Training with Symmetric Model1's coefficient...")
  full_bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))]
  bitext = full_bitext[:opts.num_sents]
  iter_num = opts.num_iters
  t_ef = defaultdict(float)
  t_fe = defaultdict(float)
  
  f_e_map = defaultdict(set)
  e_f_map = defaultdict(set)
  for (_, (f, e)) in enumerate(bitext):
    for word_e in e:
        for word_f in f:
          f_e_map[word_f].update(e)
          e_f_map[word_e].update(f)
  for (_, (f, e)) in enumerate(bitext):
    for word_e in e:
        for word_f in f:
          t_ef[(word_e,word_f)] = 1/len(f_e_map[word_f])
          t_fe[(word_f,word_e)] = 1/len(e_f_map[word_e])
  
  for iter in range(iter_num):
    sys.stderr.write("\n iter: {}/{}".format(iter+1, iter_num))
    sys.stderr.write("\n Updating count_ef and count_fe ")
    total_f = defaultdict(float)
    total_e = defaultdict(float)
    count_ef = defaultdict(float)
    count_fe = defaultdict(float)
    s_total_e = defaultdict(float)
    s_total_f = defaultdict(float)
    for (n, (f, e)) in enumerate(bitext):
      for word_e in e:
        for word_f in f:
          s_total_e[word_e] += t_ef[(word_e,word_f)]
          s_total_f[word_f] += t_fe[(word_f,word_e)]
      for word_e in e:
        for word_f in f:
          ef_prob = t_ef[(word_e,word_f)]/(s_total_e[word_e])
          count_ef[(word_e,word_f)] += ef_prob
          total_f[(word_f)] += ef_prob
          fe_prob = t_fe[(word_f,word_e)]/(s_total_f[word_f])
          count_fe[(word_f,word_e)] += fe_prob
          total_e[(word_e)] += fe_prob
    
    sys.stderr.write("\n Updating t_ef and t_fe ")
    for (word_e, word_f) in count_ef.keys():
      t_ef[(word_e,word_f)] = count_ef[(word_e,word_f)]/(total_f[(word_f)])
    for (word_f, word_e) in count_fe.keys():
      t_fe[(word_f,word_e)] = count_fe[(word_f,word_e)]/(total_e[(word_e)])
    sys.stderr.write("\n")

  for (f, e) in full_bitext:
    for (i, f_i) in enumerate(f): 
      for (j, e_j) in enumerate(e):
        if t_ef[(e_j,f_i)] >= opts.threshold and t_fe[(f_i,e_j)] >= opts.threshold:
          sys.stdout.write("%i-%i " % (i,j))
    sys.stdout.write("\n")

if opts.mode == 1:
  train_model1()
elif opts.mode == 2:
  train_symmetric_model1()
else: 
  train_dice()
